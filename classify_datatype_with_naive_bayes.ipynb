{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project DataType\n",
    "\n",
    "Les étapes:\n",
    "\n",
    "- lire le fichier data_for_project_datatype.txt\n",
    "- extraire les données entrées/sorties (liste de documents et liste des classes)\n",
    "- on construit le pipeline\n",
    "    - on va tester plusieurs vectorizers: CountVectorizer (N-grams, Binary ou pas, etc), TFIDFVectorizer. On utilise la granularité du caractère.\n",
    "    - on utilise le Bayesian classifier: MultinomialNB.\n",
    "- on vérifie à l'oeil si ca marche ou pas\n",
    "- on fait varier les hyperparamètres pour trouver le meilleur score (cross validation score).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368778 données\n",
      "données entrainement: 295022\n",
      "données de test     : 73756\n",
      "start training\n",
      "training done !\n",
      "correct ones 73605\n",
      "false ones 151\n",
      "ratio: 0.9979527089321547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "ref = {\n",
    "    \"0\": \"float\",\n",
    "    \"1\": \"int\",\n",
    "    \"2\": \"Code postal\",\n",
    "    \"3\": \"Coordonnées GPS\",\n",
    "    \"4\": \"Adresse\",\n",
    "    \"5\": \"SIRET\",\n",
    "    \"6\": \"Année\",\n",
    "    \"7\": \"Date\",\n",
    "    \"8\": \"SIREN\",\n",
    "    \"9\": \"Code NAF\",\n",
    "    \"10\": \"Autre\",\n",
    "    \"11\": \"Telephone\",\n",
    "}\n",
    "\n",
    "y_vector = []\n",
    "raw_documents_vector = []\n",
    "\n",
    "file_name = \"./data/project_data_type/data_for_project_datatype.txt\"\n",
    "with open(file_name, \"rt\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "        data = line.split(\" \", maxsplit=1)\n",
    "        y = data[0]\n",
    "        text = data[1].strip()\n",
    "        raw_document = str(len(text)) + \" \" + text\n",
    "        # raw_document = text\n",
    "        \n",
    "        # if i%1000 == 0:\n",
    "        #    print(ref[y], raw_document)\n",
    "        # raw_document = line[2:]\n",
    "        # je garde les données dans des listes\n",
    "        if y != \"\" and raw_document != \"\":\n",
    "            y_vector.append(y)\n",
    "            raw_documents_vector.append(raw_document)\n",
    "\n",
    "print(len(y_vector), \"données\")\n",
    "\n",
    "# on construit le pipeline\n",
    "NGRAM_RANGE = (1, 3)\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    input=\"content\", \n",
    "    analyzer=\"char_wb\", \n",
    "    ngram_range=NGRAM_RANGE, \n",
    "    stop_words=None,\n",
    "    binary=True)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    input=\"content\", \n",
    "    analyzer=\"char\", \n",
    "    ngram_range=NGRAM_RANGE, \n",
    "    stop_words=None\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([(\"vectorizer\", count_vectorizer), (\"classifier\", MultinomialNB())])\n",
    "\n",
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('features', FeatureUnion([\n",
    "#         ('text', Pipeline([\n",
    "#             ('vectorizer', count_vectorizer),\n",
    "#         ])),\n",
    "#         ('length', Pipeline([\n",
    "#             ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "#         ]))\n",
    "#     ])),\n",
    "#     ('clf', MultinomialNB() )])\n",
    "\n",
    "# on divise nos données en échantillon d'entrainement et de test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    raw_documents_vector, \n",
    "    y_vector, \n",
    "    shuffle=True, \n",
    "    test_size=0.2, \n",
    "    train_size=None)\n",
    "\n",
    "print(\"données entrainement:\", len(X_train))\n",
    "print(\"données de test     :\", len(X_test))\n",
    "\n",
    "# on entraine le pipeline\n",
    "print(\"start training\")\n",
    "pipeline.fit(X_train, Y_train)\n",
    "print(\"training done !\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# on prédit sur l'échantillon de test\n",
    "predicted = pipeline.predict(X_test)\n",
    "\n",
    "print(\"correct ones\", np.sum(predicted == Y_test))\n",
    "print(\"false ones\", np.sum(predicted != Y_test))\n",
    "print(\"ratio:\", np.mean(predicted == Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on test: 0.9979527089321547\n",
      "score on train: 0.9985221441112866\n"
     ]
    }
   ],
   "source": [
    "print(\"score on test:\", pipeline.score(X_test, Y_test))\n",
    "print(\"score on train:\", pipeline.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "8\n",
      " 8\n",
      "8 \n",
      " 8 \n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "9\n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "# on affiche N mots du vocabulaire\n",
    "i_max = 10\n",
    "for i, word in enumerate(count_vectorizer.vocabulary_.keys()):\n",
    "    if i > i_max:\n",
    "        break\n",
    "    else:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(np.where(predicted != Y_test))[0]:\n",
    "    pass\n",
    "    # print(i, \"|\", X_test[i][0:100].strip(), \"|\", ref[str(Y_test[i])],\"-> predicted: \", ref[str(predicted[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 566201120\n",
      "classe: Code postal\n"
     ]
    }
   ],
   "source": [
    "data = input(\"Saisir: \")\n",
    "data = str(len(data))+\" \"+data\n",
    "print(data)\n",
    "print(\"classe:\", ref[pipeline.predict([data])[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On teste une autre vectorisation\n",
    "\n",
    "- On fixe la taille du vecteur d'entrée.\n",
    "- On enlève les espaces du texte\n",
    "- Le texte est directement mis en entrée\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358920 données\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "ref = {\n",
    "    \"0\": \"float\",\n",
    "    \"1\": \"int\",\n",
    "    \"2\": \"Code postal\",\n",
    "    \"3\": \"Coordonnées GPS\",\n",
    "    \"4\": \"Adresse\",\n",
    "    \"5\": \"SIRET\",\n",
    "    \"6\": \"Année\",\n",
    "    \"7\": \"Date\",\n",
    "    \"8\": \"SIREN\",\n",
    "    \"9\": \"Code NAF\",\n",
    "    \"10\": \"Autre\",\n",
    "    \"11\": \"Telephone\",\n",
    "}\n",
    "\n",
    "y_vector = []\n",
    "raw_documents_vector = []\n",
    "\n",
    "file_name = \"./data/project_data_type/data_ready.txt\"\n",
    "with open(file_name, \"rt\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "        data = line.split(\" \", maxsplit=1)\n",
    "        y = data[0]\n",
    "        raw_document = line\n",
    "        \n",
    "        if y != \"\" and raw_document != \"\":\n",
    "            y_vector.append(y)\n",
    "            raw_documents_vector.append(raw_document)\n",
    "\n",
    "print(len(y_vector), \"données\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 435315742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_documents_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(max([len(doc) for doc in raw_documents_vector]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31 36 17 20  4 36  3 20 36 19  4 12 15 11  4 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50]\n"
     ]
    }
   ],
   "source": [
    "# on écrit en dur la taille des données\n",
    "# le nombre de caractères max dans notre texte\n",
    "N_FEATURES = 50\n",
    "# le nombre de documents dans le corpus\n",
    "N_SAMPLES = len(raw_documents_vector)\n",
    "\n",
    "# pour convertir les caractères en entier\n",
    "CHARS = \"abcdefghijklmnopqrstuvwxyz0123456789 .,-/\"\n",
    "\n",
    "DEFAULT_IF_ABSENT = 50\n",
    "\n",
    "# on prépare la matrice d'entiers N_SAMPLES lignes et N_FEATURES colonnes\n",
    "X = np.empty(shape=(N_SAMPLES, N_FEATURES), dtype=int)\n",
    "X.fill(DEFAULT_IF_ABSENT)\n",
    "# on prépare la sortie: le vecteur des différentes classes\n",
    "y = [None]*len(raw_documents_vector)\n",
    "\n",
    "def vectorize(text):\n",
    "    text = text.lower().strip()\n",
    "    vec = np.empty(shape=N_FEATURES, dtype=int)\n",
    "    vec.fill(DEFAULT_IF_ABSENT)\n",
    "    for i_feature, carac in enumerate(text):\n",
    "        char_index = CHARS.find(carac)\n",
    "        if char_index > -1:\n",
    "            # on a trouvé le caractère\n",
    "            vec[i_feature] = char_index\n",
    "        else:\n",
    "            # on met une valeur \"VIDE\"\n",
    "            vec[i_feature] = DEFAULT_IF_ABSENT\n",
    "    return vec[:]\n",
    "\n",
    "print(vectorize(\"5 rue du temple\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "doc 8 435315742\n",
      "\n",
      "y 8\n",
      "text 435315742\n",
      "\n",
      "X [30 29 31 29 27 31 33 30 28 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50]\n",
      "--------------------------------------------------\n",
      "doc 4 5 RUE DE LA PERRIERE\n",
      "\n",
      "y 4\n",
      "text 5 RUE DE LA PERRIERE\n",
      "\n",
      "X [31 36 17 20  4 36  3  4 36 11  0 36 15  4 17 17  8  4 17  4 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50]\n",
      "--------------------------------------------------\n",
      "doc 3 1.07184 49.33819\n",
      "\n",
      "y 3\n",
      "text 1.07184 49.33819\n",
      "\n",
      "X [27 37 26 33 27 34 30 36 30 35 37 29 29 34 27 35 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50]\n",
      "--------------------------------------------------\n",
      "doc 3 5.32313 43.36364\n",
      "\n",
      "y 3\n",
      "text 5.32313 43.36364\n",
      "\n",
      "X [31 37 29 28 29 27 29 36 30 29 37 29 32 29 32 30 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# on boucle sur les documents du corpus\n",
    "for i_sample, doc in enumerate(raw_documents_vector):\n",
    "    # on récupère la classe du document\n",
    "    doc_class = None\n",
    "    # y[i_sample] = doc[0:2].strip()\n",
    "    \n",
    "    # on vectorize le text\n",
    "    X[i_sample, :] = vectorize(doc[2:])\n",
    "\n",
    "    if i_sample%100000 == 0:\n",
    "        print(\"-\"*50)\n",
    "        print(\"doc\", doc)\n",
    "        print(\"y\", y[i_sample])\n",
    "        print(\"text\", doc[2:])\n",
    "        print(\"X\", X[i_sample])\n",
    "\n",
    "y = np.array(y, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On divise notre échantillon total en une sous partie pour l'apprentissage\n",
    "# et une sous partie pour le test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    shuffle=True, \n",
    "    test_size=0.3, \n",
    "    train_size=None)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107676 93372\n",
      "good ratio 0.8671570266354619\n",
      "[[28 26 27 ... 50 50 50]\n",
      " [28 29 35 ... 50 50 50]\n",
      " [28 26 27 ... 50 50 50]\n",
      " ...\n",
      " [28 26 27 ... 50 50 50]\n",
      " [28 26 27 ... 50 50 50]\n",
      " [28 26 27 ... 50 50 50]]\n"
     ]
    }
   ],
   "source": [
    "predictions = nb.predict(X_test)\n",
    "\n",
    "total_good = (predictions == Y_test).sum()\n",
    "total = Y_test.shape[0]\n",
    "print(total, total_good)\n",
    "print(\"good ratio\", total_good/float(total))\n",
    "\n",
    "errors_index = np.where(predictions != Y_test)\n",
    "for error_index in errors_index:\n",
    "    print(X_test[error_index, :])\n",
    "\n",
    "# output = nb.predict(np.transpose(vectorize(\"place super 49756 angers\")[:, np.newaxis]))\n",
    "# print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
